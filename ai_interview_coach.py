# ai_interview_coach.py - AI ê¸°ë°˜ ë©´ì ‘ ì½”ì¹˜ ì‹œìŠ¤í…œ
from flask import Flask, render_template, Response, request, jsonify
from flask_socketio import SocketIO, emit
import cv2
import numpy as np
import mediapipe as mp
import time
import threading
from collections import deque
import math
from queue import Queue, Empty
from datetime import datetime
import json
import dotenv
from openai_advisor import InterviewAdvisor
import os

advisor = None

dotenv.load_dotenv('.env.keys', override=True)

app = Flask(__name__)

def init_ai_advisor():
    """AI ì¡°ì–¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global advisor
    try:
        advisor = InterviewAdvisor(
            api_key=os.getenv('AZURE_OPENAI_KEY'),        # ì‹¤ì œ í‚¤ ì´ë¦„
            endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
            deployment_name=os.getenv('AZURE_OPENAI_MODEL', 'gpt-4')  # ì‹¤ì œ í‚¤ ì´ë¦„
        )
        print("ğŸ¤– AI ì¡°ì–¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ AI ì¡°ì–¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# Socket.IO ìµœì í™” ì„¤ì •
socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    async_mode='threading',
    ping_timeout=30,
    ping_interval=10,
    logger=True,
    engineio_logger=True
)

# MediaPipe ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™”)
mp_face_mesh = mp.solutions.face_mesh
mp_pose = mp.solutions.pose
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1,
    refine_landmarks=False,
    min_detection_confidence=0.3,
    min_tracking_confidence=0.3
)

pose = mp_pose.Pose(
    static_image_mode=False,
    model_complexity=0,  # ìµœì†Œ ë³µì¡ë„
    min_detection_confidence=0.3,
    min_tracking_confidence=0.3
)

# ì† ì œìŠ¤ì³ëŠ” ì„ íƒì  í™œì„±í™” (ì„±ëŠ¥ ê³ ë ¤)
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.7,  # ì„ê³„ê°’ ë†’ì—¬ì„œ ì—°ì‚° ê°ì†Œ
    min_tracking_confidence=0.7
)

# í í¬ê¸° ìµœì í™”
frame_queue = Queue(maxsize=2)  # 3â†’2ë¡œ ì¶•ì†Œ
metrics_queue = Queue(maxsize=5)  # 10â†’5ë¡œ ì¶•ì†Œ

class OneEuroFilter:
    def __init__(self, min_cutoff=1.0, beta=0.01):
        self.min_cutoff = min_cutoff
        self.beta = beta
        self.dx_prev = 0.0
        self.x_prev = None
        self.timestamp_prev = None

    def filter(self, x, timestamp):
        if self.x_prev is None:
            self.x_prev = x
            self.timestamp_prev = timestamp
            return x

        dt = timestamp - self.timestamp_prev
        if dt <= 0:
            return x

        dx = (x - self.x_prev) / dt
        cutoff = self.min_cutoff + self.beta * abs(dx)
        alpha = 1.0 / (1.0 + (1.0 / (2.0 * np.pi * cutoff * dt)))
        x_filtered = alpha * x + (1.0 - alpha) * self.x_prev
        
        self.x_prev = x_filtered
        self.dx_prev = dx
        self.timestamp_prev = timestamp
        
        return x_filtered

# í—¤ë“œ í¬ì¦ˆ í•„í„°
pitch_filter = OneEuroFilter(min_cutoff=0.1, beta=0.01)
yaw_filter = OneEuroFilter(min_cutoff=0.1, beta=0.01)
roll_filter = OneEuroFilter(min_cutoff=0.1, beta=0.01)

class InterviewMetrics:
    def __init__(self):
        # ================================
        # ğŸ¯ ì¡°ì • ê°€ëŠ¥í•œ ì„ê³„ê°’ë“¤ (ìˆ˜ì •ê¸ˆì§€ - ìµœì í™”ë¨)
        # ================================
        
        # ğŸ‘ ëˆˆ ê¹œë¹¡ì„ ê´€ë ¨ ì„ê³„ê°’
        self.ear_threshold = 0.12           # EAR ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë¯¼ê°, 0.15~0.25 ê¶Œì¥)
        self.ear_consecutive_frames = 1     # ê¹œë¹¡ì„ ì¸ì •ì„ ìœ„í•œ ì—°ì† í”„ë ˆì„ ìˆ˜ (1~3 ê¶Œì¥)
        self.ear_calibration_frames = 30    # ê°œì¸ë³„ ë³´ì •ì„ ìœ„í•œ í”„ë ˆì„ ìˆ˜ (20~50 ê¶Œì¥)
        
        # ğŸ¯ ë¨¸ë¦¬ ë„ë•ì„/í”ë“¤ê¸° ê´€ë ¨ ì„ê³„ê°’
        self.nod_threshold = 15             # ë„ë•ì„ ê°ì§€ ê°ë„ ì„ê³„ê°’ (15~25ë„ ê¶Œì¥)
        self.shake_threshold = 25           # ì¢Œìš° í”ë“¤ê¸° ê°ì§€ ê°ë„ ì„ê³„ê°’ (20~35ë„ ê¶Œì¥)
        self.nod_cooldown = 1.0            # ë„ë•ì„ ê°ì§€ í›„ ëŒ€ê¸°ì‹œê°„(ì´ˆ) (0.5~2.0 ê¶Œì¥)
        self.shake_cooldown = 1.0          # í”ë“¤ê¸° ê°ì§€ í›„ ëŒ€ê¸°ì‹œê°„(ì´ˆ) (0.5~2.0 ê¶Œì¥)
        self.head_min_frames = 8           # ë¨¸ë¦¬ ì›€ì§ì„ íŒë‹¨ ìµœì†Œ í”„ë ˆì„ ìˆ˜ (5~15 ê¶Œì¥)
        self.nod_pattern_frames = 4        # ë„ë•ì„ íŒ¨í„´ ë¶„ì„ í”„ë ˆì„ ìˆ˜ (3~6 ê¶Œì¥)
        
        # ğŸ“± ìì„¸ í”ë“¤ë¦¼ ê´€ë ¨ ì„ê³„ê°’  
        self.movement_threshold = 0.06      # ì–´ê¹¨ ì›€ì§ì„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë¯¼ê°, 0.02~0.1 ê¶Œì¥)
        self.posture_min_frames = 10       # ìì„¸ íŒë‹¨ ìµœì†Œ í”„ë ˆì„ ìˆ˜ (5~20 ê¶Œì¥)
        self.posture_sway_cooldown = 2.0   # ìì„¸ í”ë“¤ë¦¼ ê°ì§€ í›„ ëŒ€ê¸°ì‹œê°„(ì´ˆ) (1~5 ê¶Œì¥)
        
        # ğŸ˜Š ë¯¸ì†Œ ê´€ë ¨ ì„ê³„ê°’
        self.smile_threshold = 5           # ë¯¸ì†Œ ì¸ì • ì„ê³„ê°’ (3~10 ê¶Œì¥)
        self.smile_intensity_high = 10     # ê°•í•œ ë¯¸ì†Œ ì„ê³„ê°’ (8~15 ê¶Œì¥)
        
        # ğŸ‘‹ ì† ì œìŠ¤ì³ ê´€ë ¨ ì„ê³„ê°’ (ì„±ëŠ¥ ìµœì í™”ë¨)
        self.hand_gesture_threshold = 0.03  # ì† ì›€ì§ì„ ì„ê³„ê°’ (ì•½ê°„ ë†’ê²Œ ì„¤ì •)
        self.hand_gesture_cooldown = 1.5   # ì œìŠ¤ì³ ê°ì§€ í›„ ëŒ€ê¸°ì‹œê°„ (ê¸¸ê²Œ ì„¤ì •)
        self.hand_min_frames = 6           # ì œìŠ¤ì³ íŒë‹¨ ìµœì†Œ í”„ë ˆì„ ìˆ˜
        self.hand_analysis_interval = 5    # 5í”„ë ˆì„ë§ˆë‹¤ ì† ë¶„ì„ (ì„±ëŠ¥ ìµœì í™”)
        
        # ğŸš€ ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.enable_hand_analysis = True   # ì† ë¶„ì„ í™œì„±í™”/ë¹„í™œì„±í™”
        self.frame_skip_base = 2           # ê¸°ë³¸ í”„ë ˆì„ ìŠ¤í‚µ
        self.frame_skip_with_hands = 3     # ì† ë¶„ì„ ì‹œ í”„ë ˆì„ ìŠ¤í‚µ
        
        # ================================
        # ğŸ•’ AI ë©´ì ‘ ì½”ì¹˜ ì „ìš© ë³€ìˆ˜ë“¤
        # ================================
        
        # ë¶„ì„ ì„¸ì…˜ ì‹œê°„ ì¶”ì 
        self.analysis_start_time = None     # ë¶„ì„ ì‹œì‘ ì‹œê°„
        self.analysis_end_time = None       # ë¶„ì„ ì¢…ë£Œ ì‹œê°„
        self.session_duration = 0           # ì´ ì„¸ì…˜ ì‹œê°„
        
        # ================================
        # ì‹œìŠ¤í…œ ë³€ìˆ˜ë“¤ (ê±´ë“œë¦¬ì§€ ë§ˆì„¸ìš”!)
        # ================================
        
        # ë¶„ì„ ì œì–´
        self.analyzing = False
        self.frame_skip = 0
        self.skip_interval = self.frame_skip_base
        
        # ëˆˆ ê¹œë¹¡ì„ ìƒíƒœ ë³€ìˆ˜
        self.blink_count = 0
        self.last_blink_time = 0
        self.ear_below = False
        self.ear_closed_frames = 0
        self.ear_history_for_calibration = deque(maxlen=self.ear_calibration_frames)
        self.is_ear_calibrated = False
        self.dynamic_ear_threshold = self.ear_threshold
        
        # ë¨¸ë¦¬ ìì„¸ ìƒíƒœ ë³€ìˆ˜
        self.head_pose_history = deque(maxlen=15)
        self.nod_count = 0
        self.shake_count = 0
        self.last_nod_time = 0
        self.last_shake_time = 0
        self.pitch_history = deque(maxlen=5)
        self.yaw_history = deque(maxlen=5)
        
        # ë¯¸ì„¸ í‘œì • ìƒíƒœ ë³€ìˆ˜
        self.emotion_history = deque(maxlen=5)
        self.micro_expression_count = 0
        
        # ë¯¸ì†Œ ìƒíƒœ ë³€ìˆ˜
        self.smile_start_time = None
        self.smile_duration = 0
        self.total_smile_time = 0
        self.smile_intensity_history = deque(maxlen=30)
        
        # ìì„¸ ìƒíƒœ ë³€ìˆ˜
        self.shoulder_positions = deque(maxlen=30)
        self.posture_sway_count = 0
        self.last_sway_time = 0
        
        # ì† ì œìŠ¤ì³ ìƒíƒœ ë³€ìˆ˜ (ìµœì í™”ë¨)
        self.hand_positions = deque(maxlen=15)  # í¬ê¸° ì¶•ì†Œ
        self.hand_gesture_count = 0
        self.last_gesture_time = 0
        self.hands_visible_count = 0
        self.hand_frame_counter = 0  # ì† ë¶„ì„ ì£¼ê¸° ì¹´ìš´í„°
        
        # í†µê³„ ë³€ìˆ˜
        self.frame_count = 0
        self.start_time = time.time()
        self.processing_active = True

metrics = InterviewMetrics()

def calibrate_ear_threshold():
    """ë™ì  EAR ì„ê³„ê°’ ë³´ì •"""
    global metrics
    
    if len(metrics.ear_history_for_calibration) >= metrics.ear_calibration_frames:
        ear_values = list(metrics.ear_history_for_calibration)
        mean_ear = np.mean(ear_values)
        std_ear = np.std(ear_values)
        
        metrics.dynamic_ear_threshold = max(0.15, mean_ear - 2*std_ear)
        metrics.is_ear_calibrated = True
        
        print(f"ğŸ¯ EAR ì„ê³„ê°’ ë³´ì • ì™„ë£Œ: {metrics.dynamic_ear_threshold:.3f}")

def calculate_ear(landmarks, eye_indices):
    """ëˆˆ ì¢…íš¡ë¹„(EAR) ê³„ì‚°"""
    try:
        coords = np.array([[landmarks[i].x, landmarks[i].y] for i in eye_indices])
        A = np.linalg.norm(coords[1] - coords[5])
        B = np.linalg.norm(coords[2] - coords[4])
        C = np.linalg.norm(coords[0] - coords[3])
        return (A + B) / (2.0 * C) if C > 0 else 0
    except (IndexError, ZeroDivisionError):
        return 0

def calculate_head_pose(landmarks, img_size):
    """ë¨¸ë¦¬ ìì„¸ ê°ë„ ê³„ì‚° - One Euro Filter ì ìš©"""
    try:
        h, w = img_size
        
        model_points = np.array([
            (0.0, 0.0, 0.0),
            (0.0, -330.0, -65.0),
            (-225.0, 170.0, -135.0),
            (225.0, 170.0, -135.0),
            (-150.0, -150.0, -125.0),
            (150.0, -150.0, -125.0)
        ], dtype=np.float64)
        
        image_points = np.array([
            [landmarks[1].x * w, landmarks[1].y * h],
            [landmarks[175].x * w, landmarks[175].y * h],
            [landmarks[33].x * w, landmarks[33].y * h],
            [landmarks[263].x * w, landmarks[263].y * h],
            [landmarks[61].x * w, landmarks[61].y * h],
            [landmarks[291].x * w, landmarks[291].y * h]
        ], dtype=np.float64)
        
        focal_length = w
        camera_matrix = np.array([
            [focal_length, 0, w/2],
            [0, focal_length, h/2],
            [0, 0, 1]
        ], dtype=np.float64)
        
        dist_coeffs = np.zeros((4, 1))
        
        success, rotation_vector, translation_vector = cv2.solvePnP(
            model_points, image_points, camera_matrix, dist_coeffs
        )
        
        if success:
            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
            sy = math.sqrt(rotation_matrix[0,0]**2 + rotation_matrix[1,0]**2)
            
            if sy > 1e-6:
                x = math.atan2(rotation_matrix[2,1], rotation_matrix[2,2])
                y = math.atan2(-rotation_matrix[2,0], sy)
                z = math.atan2(rotation_matrix[1,0], rotation_matrix[0,0])
            else:
                x = math.atan2(-rotation_matrix[1,2], rotation_matrix[1,1])
                y = math.atan2(-rotation_matrix[2,0], sy)
                z = 0
            
            pitch, yaw, roll = math.degrees(x), math.degrees(y), math.degrees(z)
            
            # One Euro Filter ì ìš©
            current_time = time.time()
            filtered_pitch = pitch_filter.filter(pitch, current_time)
            filtered_yaw = yaw_filter.filter(yaw, current_time)
            filtered_roll = roll_filter.filter(roll, current_time)
            
            return filtered_pitch, filtered_yaw, filtered_roll
            
    except Exception as e:
        print(f"Head pose calculation error: {e}")
        return 0, 0, 0

def is_relaxed_nod_pattern(pitch_sequence):
    """ì™„í™”ëœ ë„ë•ì„ íŒ¨í„´ ê²€ì¦"""
    if len(pitch_sequence) < metrics.nod_pattern_frames:
        return False
    
    pitch_range = max(pitch_sequence) - min(pitch_sequence)
    if pitch_range < 12:
        return False
    
    direction_changes = 0
    for i in range(1, len(pitch_sequence)):
        if i > 1 and (pitch_sequence[i] - pitch_sequence[i-1]) * (pitch_sequence[i-1] - pitch_sequence[i-2]) < 0:
            direction_changes += 1
    
    return direction_changes >= 1

def is_relaxed_shake_pattern(yaw_sequence):
    """ì™„í™”ëœ í”ë“¤ê¸° íŒ¨í„´ ê²€ì¦"""
    if len(yaw_sequence) < metrics.nod_pattern_frames:
        return False
    
    yaw_range = max(yaw_sequence) - min(yaw_sequence)
    if yaw_range < 15:
        return False
    
    direction_changes = 0
    for i in range(2, len(yaw_sequence)):
        if (yaw_sequence[i] - yaw_sequence[i-1]) * (yaw_sequence[i-1] - yaw_sequence[i-2]) < 0:
            direction_changes += 1
    
    return direction_changes >= 1

def detect_nod_shake(current_pitch, current_yaw):
    """ê°œì„ ëœ ë„ë•ì„/í”ë“¤ê¸° ê°ì§€"""
    global metrics
    
    current_time = time.time()
    
    metrics.pitch_history.append(current_pitch)
    metrics.yaw_history.append(current_yaw)
    
    if len(metrics.pitch_history) < 5:
        return
    
    # ë„ë•ì„ ê°ì§€
    if current_time - metrics.last_nod_time > metrics.nod_cooldown:
        if len(metrics.head_pose_history) > metrics.head_min_frames:
            recent_pitches = [pose[0] for pose in list(metrics.head_pose_history)[-metrics.head_min_frames:]]
            pitch_range = max(recent_pitches) - min(recent_pitches)
            
            if pitch_range > metrics.nod_threshold:
                pitch_seq = recent_pitches[-metrics.nod_pattern_frames:]
                if is_relaxed_nod_pattern(pitch_seq):
                    metrics.nod_count += 1
                    metrics.last_nod_time = current_time
                    print(f"âœ… ë„ë•ì„ ê°ì§€! {metrics.nod_count}íšŒ")
    
    # ì¢Œìš° í”ë“¤ê¸° ê°ì§€
    if current_time - metrics.last_shake_time > metrics.shake_cooldown:
        if len(metrics.head_pose_history) > metrics.head_min_frames:
            recent_yaws = [pose[1] for pose in list(metrics.head_pose_history)[-metrics.head_min_frames:]]
            yaw_range = max(recent_yaws) - min(recent_yaws)
            
            if yaw_range > metrics.shake_threshold:
                yaw_seq = recent_yaws[-metrics.nod_pattern_frames:]
                if is_relaxed_shake_pattern(yaw_seq):
                    metrics.shake_count += 1
                    metrics.last_shake_time = current_time
                    print(f"âœ… ì¢Œìš° í”ë“¤ê¸° ê°ì§€! {metrics.shake_count}íšŒ")

def calculate_smile_intensity(landmarks):
    """ë¯¸ì†Œ ê°•ë„ ê³„ì‚°"""
    try:
        left_mouth = landmarks[61]
        right_mouth = landmarks[291]
        mouth_center = landmarks[13]
        
        left_lift = mouth_center.y - left_mouth.y
        right_lift = mouth_center.y - right_mouth.y
        
        smile_intensity = (left_lift + right_lift) / 2
        return max(0, smile_intensity * 1000)
    except (IndexError, AttributeError):
        return 0

def improved_posture_sway_detection(shoulder_landmarks):
    """ê°œì„ ëœ ìì„¸ í”ë“¤ë¦¼ ê°ì§€"""
    global metrics
    
    current_time = time.time()
    
    if current_time - metrics.last_sway_time < metrics.posture_sway_cooldown:
        return
    
    if not shoulder_landmarks:
        return
    
    left_shoulder = shoulder_landmarks.get(11)
    right_shoulder = shoulder_landmarks.get(12)
    
    if left_shoulder and right_shoulder:
        center_x = (left_shoulder.x + right_shoulder.x) / 2
        center_y = (left_shoulder.y + right_shoulder.y) / 2
        metrics.shoulder_positions.append((center_x, center_y))
        
        if len(metrics.shoulder_positions) > metrics.posture_min_frames:
            recent_positions = list(metrics.shoulder_positions)[-metrics.posture_min_frames:]
            x_positions = [pos[0] for pos in recent_positions]
            y_positions = [pos[1] for pos in recent_positions]
            
            x_range = max(x_positions) - min(x_positions)
            y_range = max(y_positions) - min(y_positions)
            
            if x_range > metrics.movement_threshold or y_range > metrics.movement_threshold:
                metrics.posture_sway_count += 1
                metrics.last_sway_time = current_time
                print(f"ğŸ“± ìì„¸ í”ë“¤ë¦¼ ê°ì§€! {metrics.posture_sway_count}íšŒ")

def detect_hand_gestures_optimized(hand_results):
    """ìµœì í™”ëœ ì† ì œìŠ¤ì³ ê°ì§€ (ì£¼ê¸°ì  ì‹¤í–‰)"""
    global metrics
    
    # ì„±ëŠ¥ ìµœì í™”: Ní”„ë ˆì„ë§ˆë‹¤ë§Œ ì‹¤í–‰
    metrics.hand_frame_counter += 1
    if metrics.hand_frame_counter < metrics.hand_analysis_interval:
        if hand_results.multi_hand_landmarks:
            metrics.hands_visible_count += 1
        return
    
    metrics.hand_frame_counter = 0
    current_time = time.time()
    
    if hand_results.multi_hand_landmarks:
        metrics.hands_visible_count += 1
        
        # ê°„ë‹¨í•œ ì†ëª© ìœ„ì¹˜ë§Œ ì¶”ì  (ì„±ëŠ¥ ìµœì í™”)
        for hand_landmarks in hand_results.multi_hand_landmarks:
            wrist = hand_landmarks.landmark[0]
            metrics.hand_positions.append((wrist.x, wrist.y, current_time))
        
        # ì œìŠ¤ì³ ê°ì§€ (ì¿¨ë‹¤ìš´ ì ìš©)
        if current_time - metrics.last_gesture_time > metrics.hand_gesture_cooldown:
            if len(metrics.hand_positions) > metrics.hand_min_frames:
                recent_positions = list(metrics.hand_positions)[-metrics.hand_min_frames:]
                
                if len(recent_positions) > 1:
                    x_positions = [pos[0] for pos in recent_positions]
                    y_positions = [pos[1] for pos in recent_positions]
                    
                    x_range = max(x_positions) - min(x_positions)
                    y_range = max(y_positions) - min(y_positions)
                    
                    if x_range > metrics.hand_gesture_threshold or y_range > metrics.hand_gesture_threshold:
                        metrics.hand_gesture_count += 1
                        metrics.last_gesture_time = current_time
                        print(f"ğŸ‘‹ ì† ì œìŠ¤ì³ ê°ì§€! {metrics.hand_gesture_count}íšŒ")

def get_detailed_analysis_data():
    """AI ë¶„ì„ì„ ìœ„í•œ ìƒì„¸ ë°ì´í„° ìƒì„±"""
    global metrics
    
    # ì„¸ì…˜ ì‹œê°„ ê³„ì‚°
    if metrics.analysis_start_time:
        if metrics.analysis_end_time:
            session_duration = metrics.analysis_end_time - metrics.analysis_start_time
        else:
            session_duration = time.time() - metrics.analysis_start_time
    else:
        session_duration = 0
    
    # ìƒì„¸ ë¶„ì„ ë°ì´í„°
    analysis_data = {
        'session_info': {
            'duration_seconds': round(session_duration, 1),
            'total_frames': metrics.frame_count,
            'analysis_timestamp': datetime.now().isoformat(),
            'start_time': datetime.fromtimestamp(metrics.analysis_start_time).isoformat() if metrics.analysis_start_time else None,
            'end_time': datetime.fromtimestamp(metrics.analysis_end_time).isoformat() if metrics.analysis_end_time else None
        },
        'behavioral_metrics': {
            'eye_contact': {
                'blink_count': metrics.blink_count,
                'blink_rate_per_minute': round(metrics.blink_count / (session_duration / 60), 1) if session_duration > 0 else 0,
                'average_ear': round(np.mean(list(metrics.ear_history_for_calibration)) if metrics.ear_history_for_calibration else 0, 3)
            },
            'facial_expressions': {
                'total_smile_time': round(metrics.total_smile_time, 1),
                'smile_percentage': round((metrics.total_smile_time / session_duration) * 100, 1) if session_duration > 0 else 0,
                'average_smile_intensity': round(np.mean(list(metrics.smile_intensity_history)) if metrics.smile_intensity_history else 0, 2),
                'micro_expressions': metrics.micro_expression_count
            },
            'head_movements': {
                'nod_count': metrics.nod_count,
                'shake_count': metrics.shake_count,
                'head_stability_score': calculate_head_stability()
            },
            'posture': {
                'sway_count': metrics.posture_sway_count,
                'stability_score': calculate_posture_stability()
            },
            'hand_gestures': {
                'gesture_count': metrics.hand_gesture_count,
                'hands_visible_seconds': round((metrics.hands_visible_count / 15), 1) if metrics.frame_count > 0 else 0,
                'gesture_frequency_per_minute': round(metrics.hand_gesture_count / (session_duration / 60), 1) if session_duration > 0 else 0
            }
        },
        'scientific_standards': {
            'normal_blink_rate_range': {'min': 12, 'max': 20, 'unit': 'per_minute'},
            'optimal_smile_percentage_range': {'min': 25, 'max': 40, 'unit': 'percent'},
            'appropriate_gesture_frequency_range': {'min': 2, 'max': 8, 'unit': 'per_minute'},
            'head_movement_balance': {
                'minimal_nods': 3,
                'optimal_nods': 8, 
                'excessive_nods': 15,
                'excessive_shake': 5
            }
        }
    }
    
    return analysis_data

def calculate_head_stability():
    """ë¨¸ë¦¬ ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° (0-100)"""
    if not metrics.head_pose_history:
        return 100
    
    recent_poses = list(metrics.head_pose_history)[-30:]
    if len(recent_poses) < 10:
        return 100
    
    pitches = [pose[0] for pose in recent_poses]
    yaws = [pose[1] for pose in recent_poses]
    
    pitch_std = np.std(pitches)
    yaw_std = np.std(yaws)
    
    stability = max(0, 100 - (pitch_std + yaw_std) * 2)
    return round(stability, 1)

def calculate_posture_stability():
    """ìì„¸ ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° (0-100)"""
    if not metrics.shoulder_positions:
        return 100
    
    positions = list(metrics.shoulder_positions)[-50:]
    if len(positions) < 20:
        return 100
    
    x_coords = [pos[0] for pos in positions]
    y_coords = [pos[1] for pos in positions]
    
    x_std = np.std(x_coords)
    y_std = np.std(y_coords)
    
    stability = max(0, 100 - (x_std + y_std) * 1000)
    return round(stability, 1)

# ê¸°ì¡´ í”„ë ˆì„ ì²˜ë¦¬ ë¡œì§ (process_frame í•¨ìˆ˜ ë“±) - ë™ì¼í•˜ê²Œ ìœ ì§€
def process_frame(frame):
    """í”„ë ˆì„ ì²˜ë¦¬ ë° ì§€í‘œ ê³„ì‚° - ì„±ëŠ¥ ìµœì í™”ë¨"""
    global metrics
    
    if metrics.enable_hand_analysis:
        metrics.skip_interval = metrics.frame_skip_with_hands
    else:
        metrics.skip_interval = metrics.frame_skip_base
    
    metrics.frame_skip += 1
    if metrics.frame_skip < metrics.skip_interval:
        return {}, frame
    
    metrics.frame_skip = 0
    
    try:
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, _ = frame.shape
        
        current_metrics = {
            'timestamp': time.time(),
            'blink_count': metrics.blink_count,
            'ear': 0.0,
            'head_pose': {'pitch': 0, 'yaw': 0, 'roll': 0},
            'nod_count': metrics.nod_count,
            'shake_count': metrics.shake_count,
            'emotion': 'neutral',
            'micro_expressions': metrics.micro_expression_count,
            'smile_intensity': 0,
            'smile_duration': metrics.smile_duration,
            'total_smile_time': metrics.total_smile_time,
            'posture_sway': metrics.posture_sway_count,
            'hand_gesture_count': metrics.hand_gesture_count,
            'hands_visible_count': metrics.hands_visible_count,
            'frame_count': metrics.frame_count
        }
        
        face_results = face_mesh.process(rgb_frame)
        pose_results = pose.process(rgb_frame)
        hand_results = None
        
        if metrics.enable_hand_analysis:
            hand_results = hands.process(rgb_frame)
        
        if face_results.multi_face_landmarks:
            mp_drawing.draw_landmarks(
                frame,
                face_results.multi_face_landmarks[0],
                mp_face_mesh.FACEMESH_CONTOURS,
                mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),
                mp_drawing.DrawingSpec(color=(0,0,255), thickness=1)
            )
            
            landmarks = face_results.multi_face_landmarks[0].landmark
            
            left_ear = calculate_ear(landmarks, [33, 160, 158, 133, 153, 144])
            right_ear = calculate_ear(landmarks, [362, 385, 387, 263, 373, 380])
            ear = (left_ear + right_ear) / 2
            current_metrics['ear'] = round(ear, 3)
            
            if not metrics.is_ear_calibrated:
                metrics.ear_history_for_calibration.append(ear)
                if len(metrics.ear_history_for_calibration) >= metrics.ear_calibration_frames:
                    calibrate_ear_threshold()
            
            if metrics.analyzing:
                current_threshold = metrics.dynamic_ear_threshold if metrics.is_ear_calibrated else metrics.ear_threshold
                
                if ear < current_threshold:
                    metrics.ear_closed_frames += 1
                    if not metrics.ear_below and metrics.ear_closed_frames >= metrics.ear_consecutive_frames:
                        metrics.ear_below = True
                else:
                    if metrics.ear_below and metrics.ear_closed_frames >= metrics.ear_consecutive_frames:
                        metrics.blink_count += 1
                        metrics.last_blink_time = time.time()
                        current_metrics['blink_count'] = metrics.blink_count
                        print(f"âœ… ê¹œë¹¡ì„ #{metrics.blink_count}ë²ˆ!")
                    
                    metrics.ear_below = False
                    metrics.ear_closed_frames = 0
                
                pitch, yaw, roll = calculate_head_pose(landmarks, (h, w))
                current_metrics['head_pose'] = {
                    'pitch': round(pitch, 1),
                    'yaw': round(yaw, 1),
                    'roll': round(roll, 1)
                }
                
                metrics.head_pose_history.append((pitch, yaw, roll))
                detect_nod_shake(pitch, yaw)
                current_metrics['nod_count'] = metrics.nod_count
                current_metrics['shake_count'] = metrics.shake_count
                
                smile_intensity = calculate_smile_intensity(landmarks)
                current_metrics['smile_intensity'] = round(smile_intensity, 2)
                metrics.smile_intensity_history.append(smile_intensity)
                
                if smile_intensity > metrics.smile_threshold:
                    if metrics.smile_start_time is None:
                        metrics.smile_start_time = time.time()
                    else:
                        metrics.smile_duration = time.time() - metrics.smile_start_time
                else:
                    if metrics.smile_start_time is not None:
                        metrics.total_smile_time += metrics.smile_duration
                        metrics.smile_start_time = None
                        metrics.smile_duration = 0
                
                current_metrics['smile_duration'] = round(metrics.smile_duration, 1)
                current_metrics['total_smile_time'] = round(metrics.total_smile_time, 1)
                
                if smile_intensity > metrics.smile_intensity_high:
                    emotion = 'happy'
                elif abs(pitch) > 20 or abs(yaw) > 30:
                    emotion = 'surprised'
                else:
                    emotion = 'neutral'
                
                current_metrics['emotion'] = emotion
                
                if len(metrics.emotion_history) > 0:
                    if metrics.emotion_history[-1] != emotion:
                        metrics.micro_expression_count += 1
                
                metrics.emotion_history.append(emotion)
                current_metrics['micro_expressions'] = metrics.micro_expression_count
        
        if pose_results.pose_landmarks:
            mp_drawing.draw_landmarks(
                frame,
                pose_results.pose_landmarks,
                mp_pose.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),
                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=1)
            )
            
            if metrics.analyzing:
                pose_landmarks = {}
                for idx, landmark in enumerate(pose_results.pose_landmarks.landmark):
                    pose_landmarks[idx] = landmark
                
                improved_posture_sway_detection(pose_landmarks)
                current_metrics['posture_sway'] = metrics.posture_sway_count
        
        if metrics.enable_hand_analysis and hand_results and hand_results.multi_hand_landmarks:
            for hand_landmarks in hand_results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    frame,
                    hand_landmarks,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=2),
                    mp_drawing.DrawingSpec(color=(0,255,255), thickness=2)
                )
            
            if metrics.analyzing:
                detect_hand_gestures_optimized(hand_results)
                current_metrics['hand_gesture_count'] = metrics.hand_gesture_count
                current_metrics['hands_visible_count'] = metrics.hands_visible_count
        
        if metrics.analyzing:
            metrics.frame_count += 1
            current_metrics['frame_count'] = metrics.frame_count
        
        return current_metrics, frame
        
    except Exception as e:
        print(f"Frame processing error: {e}")
        return {}, frame

def process_frame_worker():
    """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ ì²˜ë¦¬"""
    while metrics.processing_active:
        try:
            frame = frame_queue.get(timeout=1)
            if frame is None:
                break
            
            current_metrics, processed_frame = process_frame(frame)
            
            if not metrics_queue.full():
                metrics_queue.put(current_metrics)
            
            frame_queue.task_done()
            
        except Empty:
            continue
        except Exception as e:
            print(f"Frame processing worker error: {e}")
            time.sleep(0.1)

def metrics_sender_worker():
    """ë©”íŠ¸ë¦­ì„ Socket.IOë¡œ ì „ì†¡í•˜ëŠ” ë³„ë„ ìŠ¤ë ˆë“œ"""
    while metrics.processing_active:
        try:
            current_metrics = metrics_queue.get(timeout=1)
            if current_metrics:
                socketio.emit('metrics_update', current_metrics)
                metrics_queue.task_done()
        except Empty:
            continue
        except Exception as e:
            print(f"Metrics sender error: {e}")
            time.sleep(0.1)

# ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
camera = cv2.VideoCapture(0)
camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
camera.set(cv2.CAP_PROP_FPS, 15)
camera.set(cv2.CAP_PROP_BUFFERSIZE, 1)

def generate_frames():
    processing_thread = threading.Thread(target=process_frame_worker, daemon=True)
    processing_thread.start()
    
    metrics_thread = threading.Thread(target=metrics_sender_worker, daemon=True)
    metrics_thread.start()
    
    while True:
        success, frame = camera.read()
        if not success:
            break
        
        if frame_queue.full():
            try:
                _ = frame_queue.get_nowait()
            except Empty:
                pass
        
        frame_queue.put(frame.copy())
        
        ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        frame_bytes = buffer.tobytes()
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

@app.route('/')
def index():
    return render_template('interview_coach.html')

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

# ================================
# ğŸ¤– AI ë©´ì ‘ ì½”ì¹˜ ì „ìš© API
# ================================

@app.route('/api/get_analysis_data')
def get_analysis_data():
    """AI ë¶„ì„ì„ ìœ„í•œ ìƒì„¸ ë°ì´í„° ë°˜í™˜"""
    try:
        analysis_data = get_detailed_analysis_data()
        return {
            'status': 'success',
            'data': analysis_data
        }
    except Exception as e:
        print(f"Get analysis data error: {e}")
        return {'status': 'error', 'message': str(e)}

# Socket.IO ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë“¤
@socketio.on_error()
def error_handler(e):
    print(f"âŒ SocketIO Error: {e}")
    return True

@socketio.on('connect')
def handle_connect():
    print(f"ğŸŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {request.sid}")
    emit('connection_status', {'status': 'connected', 'timestamp': time.time()})

@socketio.on('disconnect')
def handle_disconnect():
    print(f"ğŸŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€: {request.sid}")

@socketio.on('toggle_analysis')
def toggle_analysis(data):
    """ë¶„ì„ ì‹œì‘/ì¤‘ì§€ í† ê¸€ + ì‹œê°„ ì¶”ì """
    try:
        print(f"ğŸš¦ ì„œë²„: TOGGLE_RECEIVED {data}")
        analyzing = data.get('analyze', False)
        
        if analyzing:
            # ë¶„ì„ ì‹œì‘
            metrics.analysis_start_time = time.time()
            metrics.analysis_end_time = None
            print(f"ğŸ¯ ë©´ì ‘ ë¶„ì„ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            # ë¶„ì„ ì¢…ë£Œ
            metrics.analysis_end_time = time.time()
            if metrics.analysis_start_time:
                session_duration = metrics.analysis_end_time - metrics.analysis_start_time
                print(f"ğŸ¯ ë©´ì ‘ ë¶„ì„ ì™„ë£Œ: ì´ {session_duration:.1f}ì´ˆ")
            
        metrics.analyzing = analyzing
        
        emit('analysis_status', {
            'analyzing': metrics.analyzing,
            'timestamp': time.time(),
            'session_duration': session_duration if not analyzing and metrics.analysis_end_time else 0
        })
        
    except Exception as e:
        print(f"Toggle analysis error: {e}")
        emit('error', {'message': f'Analysis toggle failed: {str(e)}'})

@socketio.on('toggle_hand_analysis')
def toggle_hand_analysis(data):
    """ì† ë¶„ì„ í™œì„±í™”/ë¹„í™œì„±í™”"""
    try:
        metrics.enable_hand_analysis = data.get('enable', True)
        print(f"ğŸ‘‹ ì† ë¶„ì„ ìƒíƒœ ë³€ê²½: {metrics.enable_hand_analysis}")
        
        emit('hand_analysis_status', {
            'enabled': metrics.enable_hand_analysis,
            'timestamp': time.time()
        })
        
    except Exception as e:
        print(f"Toggle hand analysis error: {e}")

@app.route('/api/reset_metrics')
def reset_metrics():
    """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
    try:
        global metrics
        old_analyzing = metrics.analyzing
        old_hand_enabled = metrics.enable_hand_analysis
        metrics = InterviewMetrics()
        metrics.analyzing = old_analyzing
        metrics.enable_hand_analysis = old_hand_enabled
        print("ğŸ”„ ë©”íŠ¸ë¦­ ì´ˆê¸°í™” ì™„ë£Œ")
        return {'status': 'success', 'message': 'Metrics reset successfully'}
    except Exception as e:
        print(f"Reset metrics error: {e}")
        return {'status': 'error', 'message': str(e)}

@app.route('/api/get_summary')
def get_summary():
    """ì„¸ì…˜ ìš”ì•½ í†µê³„"""
    try:
        elapsed_time = time.time() - metrics.start_time
        
        # ë¶„ì„ ì„¸ì…˜ ì‹œê°„ ê³„ì‚°
        if metrics.analysis_start_time:
            if metrics.analysis_end_time:
                analysis_session_time = metrics.analysis_end_time - metrics.analysis_start_time
            else:
                analysis_session_time = time.time() - metrics.analysis_start_time
        else:
            analysis_session_time = 0
        
        hands_visible_seconds = round((metrics.hands_visible_count / 15), 1) if metrics.frame_count > 0 else 0
        
        return {
            'status': 'success',
            'data': {
                'session_duration': round(elapsed_time, 1),
                'analysis_session_duration': round(analysis_session_time, 1),  # ì‹¤ì œ ë¶„ì„ ì‹œê°„
                'total_frames': metrics.frame_count,
                'blink_rate': round(metrics.blink_count / (analysis_session_time / 60), 1) if analysis_session_time > 0 else 0,
                'nod_count': metrics.nod_count,
                'shake_count': metrics.shake_count,
                'micro_expressions': metrics.micro_expression_count,
                'total_smile_time': round(metrics.total_smile_time, 1),
                'posture_sway_count': metrics.posture_sway_count,
                'hand_gesture_count': metrics.hand_gesture_count,
                'hands_visible_seconds': hands_visible_seconds,
                'hands_visible_rate': round((metrics.hands_visible_count / metrics.frame_count) * 100, 1) if metrics.frame_count > 0 else 0,
                'avg_smile_intensity': round(np.mean(list(metrics.smile_intensity_history)) if metrics.smile_intensity_history else 0, 2)
            }
        }
    except Exception as e:
        print(f"Get summary error: {e}")
        return {'status': 'error', 'message': str(e)}

@app.route('/api/health')
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        'status': 'healthy',
        'timestamp': time.time(),
        'analyzing': metrics.analyzing,
        'frame_count': metrics.frame_count,
        'hand_analysis_enabled': metrics.enable_hand_analysis
    }

@app.route('/api/generate_ai_advice', methods=['POST'])
def generate_ai_advice():
    """AI ê¸°ë°˜ ë©´ì ‘ ì¡°ì–¸ ìƒì„±"""
    try:
        global advisor
        if not advisor:
            return {
                'status': 'error',
                'message': 'AI ì¡°ì–¸ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'fallback_advice': 'ê¸°ë³¸ ì¡°ì–¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Azure OpenAI ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.'
            }
        
        analysis_data = request.get_json()
        if not analysis_data:
            return {'status': 'error', 'message': 'ë¶„ì„ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
        
        print(f"ğŸ¤– AI ì¡°ì–¸ ìƒì„± ì‹œì‘...")
        advice_result = advisor.generate_advice(analysis_data)
        
        if advice_result['status'] == 'success':
            print(f"âœ… AI ì¡°ì–¸ ìƒì„± ì„±ê³µ")
            return {
                'status': 'success',
                'advice': advice_result['advice'],
                'analysis_summary': advice_result.get('analysis_summary', {}),
                'timestamp': advice_result['timestamp']
            }
        else:
            print(f"âš ï¸ AI ì¡°ì–¸ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ì¡°ì–¸ ì œê³µ")
            return {
                'status': 'error',
                'message': advice_result.get('message', 'AI ì¡°ì–¸ ìƒì„± ì‹¤íŒ¨'),
                'fallback_advice': advice_result.get('fallback_advice', 'ê¸°ë³¸ ì¡°ì–¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            }
    except Exception as e:
        print(f"âŒ AI ì¡°ì–¸ API ì˜¤ë¥˜: {e}")
        return {
            'status': 'error',
            'message': str(e),
            'fallback_advice': 'ì„œë²„ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¡°ì–¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
        }

if __name__ == '__main__':
    print("ğŸ¤– AI ë©´ì ‘ ì½”ì¹˜ ì„œë²„ ì‹œì‘ì¤‘...")
    
    # AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    api_key = os.getenv('AZURE_OPENAI_KEY')
    endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
    
    if api_key and endpoint:
        print("âœ… Azure OpenAI ì¸ì¦ ì •ë³´ í™•ì¸ë¨")
        init_success = init_ai_advisor()
        if init_success:
            print("ğŸ¯ AI ì¡°ì–¸ ê¸°ëŠ¥ í™œì„±í™”")
        else:
            print("âš ï¸ AI ì¡°ì–¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    else:
        print("âš ï¸ Azure OpenAI ì¸ì¦ ì •ë³´ ì—†ìŒ")

    try:
        print("ğŸš€ Flask ì„œë²„ ì‹œì‘...")
        socketio.run(app, host='0.0.0.0', port=5001, debug=True, use_reloader=False)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
        metrics.processing_active = False
        camera.release()
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        metrics.processing_active = False
        camera.release()